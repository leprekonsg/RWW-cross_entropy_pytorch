{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real-World-Weighted Crossentropy Loss Function",
      "provenance": [],
      "collapsed_sections": [
        "_27cHEGCUMiB"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaoshiang/The-Real-World-Weight-Crossentropy-Loss-Function/blob/master/Real_World_Weighted_Crossentropy_Loss_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y1dlLOBjGuy",
        "colab_type": "text"
      },
      "source": [
        "# Real-World-Weighted Crossentropy Loss Function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjW5dVQjjE6d",
        "colab_type": "text"
      },
      "source": [
        "This is a companion notebook to the paper published at <<<>>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOo8TfTRUIOX",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) 2019 Yaoshiang Ho\n",
        "\n",
        "This program is free software: you can redistribute it and/or modify\n",
        "it under the terms of the GNU General Public License as published by\n",
        "the Free Software Foundation, either version 3 of the License, or\n",
        "(at your option) any later version.\n",
        "\n",
        "This program is distributed in the hope that it will be useful,\n",
        "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "GNU General Public License for more details.\n",
        "\n",
        "Contact author for exceptions. \n",
        "\n",
        "You should have received a copy of the GNU General Public License\n",
        "along with this program.  If not, see <https://www.gnu.org/licenses/>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G2rcSxFUBVY",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6B-pD4hiHyi",
        "colab_type": "code",
        "outputId": "53082ef5-6cc9-4396-c6d4-2001de2ff7f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow import set_random_seed\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import scipy as sp\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "import datetime\n",
        "\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uUw19MXj_tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch MNIST dataset\n",
        "\n",
        "mnist = fetch_openml('mnist_784', cache=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6GkBRRwkHbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Processing on MNIST data\n",
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')\n",
        "X /= 255.0\n",
        "X = X\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_27cHEGCUMiB",
        "colab_type": "text"
      },
      "source": [
        "## The Real-World-Weight Crossentropy loss functions for binary and categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTJE_umwrIRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (C) 2019 Yaoshiang Ho\n",
        "\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "#\n",
        "# Contact author for exceptions.\n",
        "#\n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "\n",
        "def create_rww_binary_crossentropy(fn_weight, fp_weight):\n",
        "  \"\"\"\n",
        "  binary_crossentropy is a special case of categorical_crossentropy.  \n",
        "  The \"all other\" column is imputed. For single-label, an implementation is provided.\n",
        "  However, this loss function is not very useful in a multi-label environment. \n",
        "  It only supports k=1. \n",
        "  \n",
        "  In future work, we will provide an implementation for one target, \n",
        "  one miscategorization. Note that this will\n",
        "  require a tensor of at lease size (k,k,2,2), where, the first 2 \n",
        "  represents T/F of target, the second 2 represents the FP/FN of label. \n",
        "\n",
        "  We may also create a fully expressive loss function, which would require some\n",
        "  efficient representation of a tensor of size (k, k, 2^k, 2^k)\n",
        "  \n",
        "  \"\"\"  \n",
        "  def binary_loss_function(target, output):\n",
        "    output = K.clip(output, K.epsilon(), 1 - K.epsilon()) \n",
        "    \n",
        "    logs = K.log(output) # shape (m, 1)\n",
        "    logs_1_sub = K.log(1-output) # shape (m, 1)\n",
        "\n",
        "    return - K.mean(target * fn_weight * logs + \n",
        "                    (1-target) * fp_weight * logs_1_sub)\n",
        "\n",
        "  return binary_loss_function\n",
        "\n",
        "def create_rww_categorical_crossentropy(k, loss_type, fn_weights=None, fp_weights=None, return_weights=False):\n",
        "  \"\"\"Real-World-Weighted crossentropy between an output tensor and a target tensor.\n",
        "  \n",
        "  The loss_types other than rww_categorical_crossentropy reimplement existing \n",
        "  functions in Keras but are not as well optimized. \n",
        "  These loss_types are usable directly, but, are more useful when calling \n",
        "  return_weights=True, which then returns fn and fp weights matrixes of size (k,k). \n",
        "  Editing those to reflect real world costs, then passing them back into \n",
        "  create_rww_crossentropy with loss_type \"rww_crossentropy\" is the recommended approach. \n",
        "\n",
        "  Example Usage: \n",
        "\n",
        "  Suppose you have three classes: cat, dog, and other.\n",
        "  \n",
        "  Cat is one-hot encoded as [1,0,0], dog as [0,1,0], other as [0,0,1]\n",
        "  \n",
        "  The the following code increases the incremental penalty of \n",
        "  mislabeling a true target 0 (cat) with a false label 1 (dog) at a cost of 99, \n",
        "  versus the default of zero. Note that the existing fn_weights also has a \n",
        "  default cost of 1 for missing the true target of 1, for a total cost of \n",
        "  100 versus the default cost of 1. \n",
        "  \n",
        "  fn_weights, fp_weights = create_rww_categorical_crossentropy(10, \"categorical_crossentropy\", return_weights=True)\n",
        "  fp_weights[0, 1] = 99\n",
        "  loss = create_rww_categorical_crossentropy(10, \"rww_crossentropy\", fn_weights, fp_weights)\n",
        "\n",
        "... \n",
        "  \n",
        "  The fn and fp weights are easy to reason about. \n",
        "  \n",
        "  fn_weights is [x1, __, __]\n",
        "                [__, x2, __]\n",
        "                [__, __, x3]\n",
        " \n",
        "  x1 represents the scale of the cost for a fn for cat, x2 for dog, and x3 for other.\n",
        "  \n",
        "  This is calculated as fn_weight * log(y_pred). \n",
        "  \n",
        "  In the case of loss_type=categorical_crossentropy, \n",
        "  x1, x2, and x3 all equal the value one. \n",
        "  All elements not on the main axis must equal zero. \n",
        "  \n",
        "  Note that fn_weights could have been represented as a vector, \n",
        "  not a matrix, however, we use a matrix to keep symmetry with \n",
        "  fp_weights, and, to prepare for \n",
        "  multi-label classification. \n",
        "    \n",
        "  ...\n",
        "\n",
        "  fp_weights is concerned with the costs of the fps from the other classes. \n",
        "\n",
        "  fp_weights of [__, x1, x2]\n",
        "                [x3, __, x4]\n",
        "                [x5, x6, __]\n",
        " \n",
        "  x1 represents the cost of predicting 1 for dog, when it should be 0 for cat. \n",
        "  x2 represents predicting 2 for other, when the target is 0 for cat. \n",
        "  x3 represents predicing 0 for cat, when the target is 1 for dog.\n",
        "  etc. \n",
        "  \n",
        "  Args:\n",
        "    * k: 2 or more for number of categories, including \"other\". \n",
        "    * loss_type: \"categorical_crossentropy\" to initialize to \n",
        "      standard softmax_crossentropy behavior, \n",
        "      or \"weighted_categorical_crossentropy\" for standard behavior, or, \n",
        "      or \"rww_crossentropy\" for full weight matrix of all possible fn/fp combinations. \n",
        "    * fn_weights: a numpy array of shape (k,k). The main diagonal can\n",
        "      contain non-zero values; all other values must be zero. \n",
        "    * fp_weights: a numpy array of shape (k,k) to define specific combinations \n",
        "      of false positive. The main diag should be zeros. \n",
        "    * return_weights: If False (default), returns cost function. If True, \n",
        "      returns fn and fp weights as np.array. \n",
        "Returns:\n",
        "    * retval: Loss function for use Keras.model.fit, or if return_weights\n",
        "      arg is True, the fn_weights and fp_weights matrixes. \n",
        "  \"\"\"\n",
        "\n",
        "  full_fn_weights = None\n",
        "  full_fp_weights = None\n",
        "\n",
        "  anti_eye = np.ones((k,k)) - np.eye(k)\n",
        "    \n",
        "  if (loss_type==\"categorical_crossentropy\"):\n",
        "    full_fn_weights = np.identity((k))\n",
        "    full_fp_weights = np.zeros((k, k)) # Softmax crossentropy ignores fp.\n",
        "\n",
        "  elif(loss_type==\"weighted_categorical_crossentropy\"):\n",
        "    full_fn_weights = np.eye(k) * fn_weights\n",
        "    full_fp_weights = np.zeros((k, k)) # softmax crossentropy ignores fp\n",
        "    \n",
        "  elif(loss_type==\"rww_crossentropy\"):\n",
        "    assert not np.count_nonzero(fn_weights * anti_eye)\n",
        "    assert not np.count_nonzero(fp_weights * np.eye(k))\n",
        "\n",
        "    full_fn_weights = fn_weights\n",
        "    # Novel piece: allow any combination of fp.\n",
        "    full_fp_weights = fp_weights\n",
        "    \n",
        "  else:\n",
        "    raise Exception(\"unknown loss_type: \" + str(loss_type))\n",
        "   \n",
        "  fn_wt = K.constant(full_fn_weights) # (k,k), always sparse along main diag. \n",
        "  fp_wt = K.constant(full_fp_weights) # (k,k), always dense except main diag. \n",
        "\n",
        "  def loss_function(target, output):\n",
        "    output = K.clip(output, K.epsilon(), 1 - K.epsilon()) \n",
        "    \n",
        "    logs = K.log(output) # shape (m, k), dense. 1 is good. \n",
        "    logs_1_sub = K.log(1-output) # shape (m, k), dense. 0 is good. \n",
        "\n",
        "    m_full_fn_weights = K.dot(target, fn_wt) # (m,k) . (k, k)\n",
        "    m_full_fp_weights = K.dot(target, fp_wt) # (m,k) . (k, k)\n",
        "\n",
        "    return - K.mean(m_full_fn_weights * logs + \n",
        "                    m_full_fp_weights * logs_1_sub)\n",
        "  \n",
        "  if (return_weights):\n",
        "    return full_fn_weights, full_fp_weights\n",
        "  else:\n",
        "    return loss_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8uIO9axuDkW",
        "colab_type": "code",
        "outputId": "190df142-facc-4205-aecd-b5d3341a1a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "# Example of categorical_crossentropy. Class behavior. \n",
        "fn_weights, fp_weights = create_rww_categorical_crossentropy(5, 'categorical_crossentropy', return_weights=True)\n",
        "print(\"softmax cross entropy weights. k=5 classes. Expected output: fn: ones, fp:zeros. \")\n",
        "print (fn_weights)\n",
        "print (fp_weights)\n",
        "\n",
        "# Example of weighted single-label categorical crossentropy. \n",
        "fn_weights, fp_weights = create_rww_categorical_crossentropy(5, 'weighted_categorical_crossentropy', np.array([1,2,3,4,5]), return_weights=True)\n",
        "print(\"weighted softmax crossentropy. k=5 classes. Expected output: fn: weights, fp: zeros. \")\n",
        "print (fn_weights)\n",
        "print (fp_weights)\n",
        "\n",
        "\n",
        "# Example of full weighted multi-label categorical crossentropy\n",
        "fn_weights, fp_weights = create_rww_categorical_crossentropy(10, \"categorical_crossentropy\", return_weights=True)\n",
        "fp_weights[5, 8] = 19\n",
        "loss = create_rww_categorical_crossentropy(10, \"rww_crossentropy\", fn_weights, fp_weights)\n",
        "print(\"real world weighted cross entropy weights. k=10 classes. If target label if 5, heavily penalize a (false positive) 8. \")\n",
        "print (fn_weights)\n",
        "print (fp_weights)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax cross entropy weights. k=5 classes. Expected output: fn: ones, fp:zeros. \n",
            "[[1.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 1.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 1.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 1.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 1.00]]\n",
            "[[0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]]\n",
            "weighted softmax crossentropy. k=5 classes. Expected output: fn: weights, fp: zeros. \n",
            "[[1.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 2.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 3.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 4.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 5.00]]\n",
            "[[0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]]\n",
            "real world weighted cross entropy weights. k=10 classes. If target label if 5, heavily penalize a (false positive) 8. \n",
            "[[1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00]]\n",
            "[[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 19.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]]\n",
            "<function create_rww_categorical_crossentropy.<locals>.loss_function at 0x7f74a7ac0268>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKD5O2moUZWi",
        "colab_type": "text"
      },
      "source": [
        "## Testing Against Imbalanced Single-Label Multiclass (softmax)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRdr4rjpk0G2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_categorical_model(loss):\n",
        "  k_y = k_x = Input(shape=(784,))\n",
        "  k_y = Dense(50, activation='relu')(k_y)\n",
        "  k_y = Dense(20, activation='relu')(k_y)\n",
        "  k_y = Dense(10, activation='softmax')(k_y)\n",
        "\n",
        "  model = Model(inputs=k_x, outputs=k_y)\n",
        "  model.compile(optimizer='adam', loss=loss, metrics=['categorical_accuracy'])\n",
        "  return model\n",
        "\n",
        "# model_a = create_model(\"categorical_crossentropy\")\n",
        "# set_random_seed(1)\n",
        "# model_a.fit(x = X_train, y = to_categorical(y_train), epochs=10, batch_size=100, validation_split=0.1)\n",
        "# print(model_a.evaluate(X_test, to_categorical(y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI-1L6elsVe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_predictions(z, y):\n",
        "  # Inputs: \n",
        "  #   z: raw output of softmax for m classes\n",
        "  #   y: target predictions\n",
        "  # Outputs:\n",
        "  #   average predictions matrix of size (z.shape[1],z.shape[1])\n",
        "  #     axis 0 are the true categories\n",
        "  #     axis 1 are the predicted categories\n",
        "  #     each cell is the average of the predictions\n",
        "  m, n = z.shape\n",
        "  y_pred = np.argmax(z, axis=1)  \n",
        "  \n",
        "  averages = np.zeros((n, n))\n",
        "  \n",
        "  for i in range(0, n):\n",
        "    mask = y == np.array(i)\n",
        "    assert mask.shape == (m,)\n",
        "    np.mean(z[mask], axis=0, out=averages[i])\n",
        "    \n",
        "  return averages\n",
        "\n",
        "# print(\"Average predictions for model_a. Rows are y, columns are y_hat\")\n",
        "# print(average_predictions(model_a.predict(X_test), y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QovfFnw4oVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def num_predictions(z, y):\n",
        "  # Inputs: \n",
        "  #   z: raw output of softmax for m classes\n",
        "  #   y: target predictions using argmax\n",
        "  # Outputs:\n",
        "  #   total predictions matrix of size (z.shape[1],z.shape[1])\n",
        "  #     axis 0 are the true categories\n",
        "  #     axis 1 are the predicted categories\n",
        "  #     each cell is the sum of the predictions\n",
        "  m, n = z.shape\n",
        "  y_pred = np.argmax(z, axis=1)  \n",
        "  \n",
        "  sums = np.zeros((n, n))\n",
        "  \n",
        "  for i in range(0, n):\n",
        "    mask = y == np.array(i)\n",
        "    assert mask.shape == (m,)    \n",
        "    np.sum(to_categorical(np.argmax(z[mask], axis=1), num_classes=10), axis=0, out=sums[i])\n",
        "  return sums\n",
        "\n",
        "# print(\"Total number of predictions for model_a. Rows are y, columns are y_hat\")\n",
        "# np.set_printoptions(formatter={'float': lambda x: \"{0:6.0f}\".format(x)})\n",
        "# print(num_predictions(model_a.predict(X_test), y_test))\n",
        "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u58yBlElVaP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate Real World Cost\n",
        "def clip_correct(preds):\n",
        "  k = preds.shape[0]\n",
        "  assert k == preds.shape[1]\n",
        "  \n",
        "  anti_eye = 1 - np.eye(k)\n",
        "  return preds * anti_eye # k,k, with the main diagonal masked out to zeros\n",
        "\n",
        "def categorical_real_world_cost(preds, fn_ws, fp_ws):\n",
        "  k = preds.shape[0]\n",
        "  assert k == preds.shape[1]\n",
        "  \n",
        "  false_preds = clip_correct(preds)\n",
        "  fn_cost = np.sum(np.sum(false_preds, axis=-1) * np.sum(fn_ws, axis=-1)) \n",
        "  fp_cost = np.sum(false_preds * fp_ws)\n",
        " \n",
        "  return (fn_cost + fp_cost) / np.sum(preds)\n",
        "\n",
        "def vector_categorical_real_world_cost(m, preds, fn_ws, fp_ws):\n",
        "  retval = np.zeros(m)\n",
        "  for i in range(0,m):\n",
        "    retval[i] = categorical_real_world_cost(preds[i], fn_ws[i], fp_ws[i])\n",
        "  return retval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuwzaTB3n75K",
        "colab_type": "code",
        "outputId": "3cb0c192-9383-47c4-b0ca-45370012ca15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "def generate_unequal_pair(i):\n",
        "  np.random.seed(i)\n",
        "  a = np.random.randint(0, 10)\n",
        "  b = np.random.randint(0,10)\n",
        "  while a == b:\n",
        "    b = np.random.randint(0,10)\n",
        "  return a, b\n",
        "\n",
        "EPOCHS = 10 # We experimentally found no significant benefit after 10 epochs\n",
        "\n",
        "# For testing purposes only. Comment the following line out in typical usage. \n",
        "# EPOCHS = 1\n",
        "\n",
        "# Repeatability\n",
        "np.random.seed(1)\n",
        "set_random_seed(1)\n",
        "\n",
        "# Quick compare 10 sample and 10 control networks\n",
        "for i in range(0,10):\n",
        "  print(datetime.datetime.now(), end = \" \")\n",
        "  # Generate random socially biased or expensive fn,fp pair, setup rww_crossentropy loss function. \n",
        "  on_label, offensive_fp = generate_unequal_pair(i)  \n",
        "  fn_weights, fp_weights = create_rww_categorical_crossentropy(10, \"categorical_crossentropy\", return_weights=True)\n",
        "  fp_weights[on_label, offensive_fp] = 19\n",
        "  loss = create_rww_categorical_crossentropy(10, \"rww_crossentropy\", fn_weights, fp_weights)\n",
        "\n",
        "  # Train control model\n",
        "  model_control = create_categorical_model('binary_crossentropy') # K/TF builtin\n",
        "  model_control.fit(x = X_train, y = to_categorical(y_train), epochs=EPOCHS, batch_size=100, validation_split=0.1, verbose=0)\n",
        "  num_pred_control = num_predictions(model_control.predict(X_test), y_test)\n",
        "  _, acc_control = model_control.evaluate(X_test, to_categorical(y_test))\n",
        "  \n",
        "  # Train experimental model with same epochs / batchsize as control model. \n",
        "  model_test = create_categorical_model(loss) # RWW version\n",
        "  model_test.fit(x = X_train, y = to_categorical(y_train), epochs=EPOCHS, batch_size=100, validation_split=0.1, verbose=0)  \n",
        "  num_pred_test = num_predictions(model_test.predict(X_test), y_test)\n",
        "  _, acc_test = model_test.evaluate(X_test, to_categorical(y_test), verbose=0)\n",
        "\n",
        "  print(\"For test (fn:fp):(\" + str(on_label) + \",\" + \\\n",
        "        str(offensive_fp) + \"): FPs from (control, experimental): (\" + \\\n",
        "        str(num_pred_control[on_label, offensive_fp]) + \",\" +\\\n",
        "        str(num_pred_test[on_label, offensive_fp]) + \\\n",
        "        \"); Accuracy from (control, experimental):\" + \\\n",
        "        '{:0.4f}'.format(acc_control) + \",\", '{:0.4f}'.format(acc_test) + \")\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-31 16:27:03.787544 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "17500/17500 [==============================] - 1s 40us/step\n",
            "For test (fn:fp):(5,0): FPs from (control, experimental): (0.0,0.0); Accuracy from (control, experimental):0.9640, 0.9621)\n",
            "17500/17500 [==============================] - 1s 42us/step\n",
            "For test (fn:fp):(5,8): FPs from (control, experimental): (9.0,3.0); Accuracy from (control, experimental):0.9643, 0.9531)\n",
            "17500/17500 [==============================] - 1s 42us/step\n",
            "For test (fn:fp):(8,6): FPs from (control, experimental): (7.0,3.0); Accuracy from (control, experimental):0.9659, 0.9619)\n",
            "17500/17500 [==============================] - 1s 46us/step\n",
            "For test (fn:fp):(8,9): FPs from (control, experimental): (7.0,6.0); Accuracy from (control, experimental):0.9653, 0.9643)\n",
            "17500/17500 [==============================] - 1s 45us/step\n",
            "For test (fn:fp):(7,5): FPs from (control, experimental): (2.0,0.0); Accuracy from (control, experimental):0.9634, 0.9627)\n",
            "17500/17500 [==============================] - 1s 44us/step\n",
            "For test (fn:fp):(3,6): FPs from (control, experimental): (2.0,0.0); Accuracy from (control, experimental):0.9615, 0.9629)\n",
            "17500/17500 [==============================] - 1s 47us/step\n",
            "For test (fn:fp):(9,3): FPs from (control, experimental): (6.0,5.0); Accuracy from (control, experimental):0.9637, 0.9629)\n",
            "17500/17500 [==============================] - 1s 48us/step\n",
            "For test (fn:fp):(4,9): FPs from (control, experimental): (30.0,16.0); Accuracy from (control, experimental):0.9623, 0.9652)\n",
            "17500/17500 [==============================] - 1s 49us/step\n",
            "For test (fn:fp):(3,4): FPs from (control, experimental): (1.0,1.0); Accuracy from (control, experimental):0.9637, 0.9620)\n",
            "17500/17500 [==============================] - 1s 50us/step\n",
            "For test (fn:fp):(5,6): FPs from (control, experimental): (14.0,5.0); Accuracy from (control, experimental):0.9610, 0.9655)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BguZppn4sMfJ",
        "colab_type": "code",
        "outputId": "e051b510-8192-434a-8c79-6dce97beae7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Train 90 control and 90 experimental models. \n",
        "\n",
        "# Repeatability\n",
        "np.random.seed(1)\n",
        "set_random_seed(1)\n",
        "\n",
        "accs = []\n",
        "fps = []\n",
        "preds = []\n",
        "ws = []\n",
        "\n",
        "for i in range(0,10):\n",
        "  for j in range (0,10):\n",
        "    if i == j: \n",
        "      continue\n",
        "\n",
        "    print(str(i*10 + j) + \":\" + str(datetime.datetime.now()))\n",
        "\n",
        "    fn_weights, fp_weights = create_rww_categorical_crossentropy(10, \"categorical_crossentropy\", return_weights=True)\n",
        "    fp_weights[i, j] = 19\n",
        "    loss = create_rww_categorical_crossentropy(10, \"rww_crossentropy\", fn_weights, fp_weights)\n",
        "\n",
        "    # Train control model\n",
        "    model_control = create_categorical_model('binary_crossentropy') # K/TF builtin\n",
        "    model_control.fit(x = X_train, y = to_categorical(y_train), epochs=EPOCHS, batch_size=100, validation_split=0.1, verbose=0)\n",
        "    num_pred_control = num_predictions(model_control.predict(X_test, verbose=0), y_test)\n",
        "    _, acc_control = model_control.evaluate(X_test, to_categorical(y_test), verbose=0)\n",
        " \n",
        "    # Train experimental model with same epochs / batchsize as control model. \n",
        "    model_test = create_categorical_model(loss) # RWW version\n",
        "    model_test.fit(x = X_train, y = to_categorical(y_train), epochs=EPOCHS, batch_size=100, validation_split=0.1, verbose=0)  \n",
        "    num_pred_test = num_predictions(model_test.predict(X_test, verbose=0), y_test)\n",
        "    _, acc_test = model_test.evaluate(X_test, to_categorical(y_test), verbose=0)\n",
        "\n",
        "    accs.append((acc_control, acc_test))\n",
        "    fps.append((num_pred_control[i, j], num_pred_test[i,j]))\n",
        "    preds.append((num_pred_control, num_pred_test))\n",
        "    ws.append((fn_weights, fp_weights))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1:2019-10-31 16:34:28.411562\n",
            "2:2019-10-31 16:35:15.641499\n",
            "3:2019-10-31 16:36:04.309588\n",
            "4:2019-10-31 16:36:53.598391\n",
            "5:2019-10-31 16:37:44.013906\n",
            "6:2019-10-31 16:38:35.755103\n",
            "7:2019-10-31 16:39:27.471340\n",
            "8:2019-10-31 16:40:19.238765\n",
            "9:2019-10-31 16:41:11.398103\n",
            "10:2019-10-31 16:42:04.525346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALM2izF7lQVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform t-test on results (number of mislabels). We use Welch's test since the variances are different. \n",
        "\n",
        "fps = np.array(fps)\n",
        "statistic, pvalue = sp.stats.ttest_rel(fps[:,0], fps[:,1])\n",
        "\n",
        "print(np.mean(fps[:,0]))\n",
        "print(np.mean(fps[:,1]))\n",
        "\n",
        "print(statistic)\n",
        "print(pvalue)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIP-u94mpcWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform t-test on errors\n",
        "\n",
        "accs = np.array(accs)\n",
        "statistic, pvalue = sp.stats.ttest_rel(1. - accs[:,0], 1. - accs[:,1])\n",
        "\n",
        "print(np.mean(1. - accs[:,0]))\n",
        "print(np.mean(1. - accs[:,1]))\n",
        "\n",
        "print(statistic)\n",
        "print(pvalue)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWEL1WvxXMny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform t-test on real world cost\n",
        "    \n",
        "fn_ws = np.array(ws)[:,0,:,:] # size (90, k, k)\n",
        "fp_ws = np.array(ws)[:,1,:,:] # size (90, k, k)\n",
        "\n",
        "preds_c = np.array(preds)[:,0,:,:] # size (90, k, k)\n",
        "preds_t = np.array(preds)[:,1,:,:] # size (90, k, k)\n",
        "\n",
        "costs_c = vector_categorical_real_world_cost(90, preds_c, fn_ws, fp_ws)\n",
        "costs_t = vector_categorical_real_world_cost(90, preds_t, fn_ws, fp_ws)\n",
        "\n",
        "assert costs_c.shape == (90,)\n",
        "assert costs_t.shape == (90,)\n",
        "\n",
        "print(np.mean(costs_c))\n",
        "print(np.mean(costs_t))\n",
        "\n",
        "statistic, pvalue = sp.stats.ttest_rel(costs_c, costs_t)\n",
        "print(statistic)\n",
        "print(pvalue)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pko_2OWPsnIG",
        "colab_type": "text"
      },
      "source": [
        "# Testing against imbalanced binary data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTjERIGrBzP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_imbalanced_mnist_data(X, y, target, num_minority, batch=0):\n",
        "  new_X, new_y = [], []\n",
        "  \n",
        "  mask = y == target\n",
        "  anti_mask = y != target\n",
        "\n",
        "  # Grab the target X\n",
        "  X_label = X[mask]  \n",
        "      \n",
        "  # Reduce the number of target x\n",
        "  start = batch * num_minority\n",
        "  end = start + num_minority\n",
        "  X_label = X_label[start:end,:]\n",
        "  \n",
        "  new_X = np.concatenate([X_label, X[anti_mask]])\n",
        "  new_y = np.concatenate([np.ones((num_minority)), np.zeros(X[anti_mask].shape[0])])\n",
        "  \n",
        "  return new_X, new_y\n",
        "\n",
        "def create_binary_model(loss):\n",
        "  k_y = k_x = Input(shape=(784,))\n",
        "  k_y = Dense(10, activation='relu')(k_y)\n",
        "  k_y = Dense(1, activation='sigmoid')(k_y)\n",
        "\n",
        "  model = Model(inputs=k_x, outputs=k_y)\n",
        "  model.compile(optimizer='adam', loss=loss, metrics=['binary_accuracy'])\n",
        "  return model\n",
        "\n",
        "def binary_real_world_cost(fn, fp, fn_cost, fp_cost):\n",
        "  return fn*fn_cost + fp*fp_cost\n",
        "\n",
        "def binary_stats(z, y, threshold, fn_cost, fp_cost):\n",
        "  z = np.squeeze(z)\n",
        "\n",
        "  y_pred = z > threshold\n",
        "  p = np.count_nonzero(y)\n",
        "  n = np.count_nonzero(1.-y)\n",
        "\n",
        "  tp = np.count_nonzero(y_pred * y)\n",
        "  fn = np.count_nonzero((1.-y_pred) * y)\n",
        "  tn = np.count_nonzero((1.-y_pred) * (1.-y))\n",
        "  fp = np.count_nonzero(y_pred * (1.-y))\n",
        "\n",
        "  recall = np.float64(tp) / (tp + fn) \n",
        "  precision = np.float64(tp) / (tp + fp)\n",
        "  f1 = np.float64(2 * recall * precision) / (recall + precision)\n",
        "  rwc = (binary_real_world_cost (fn, fp, fn_cost, fp_cost)) / (p+n)\n",
        "  rwb = (p * fn_cost) / (p+n) - rwc\n",
        "  err = (fn + fp) / n\n",
        "  \n",
        "  \n",
        "  # XXX: This really should return a dict instead of an 12 tuple. \n",
        "  return p, n, tp, fn, tn, fp, recall, precision, f1, rwc, rwb, err\n",
        "\n",
        "# This is the algorithm to fine tune the threshold hyperparameter after training. \n",
        "def search_threshold(z, y):\n",
        "  best_t = 0;\n",
        "  best_f1 = 0\n",
        "\n",
        "  data = []\n",
        "  \n",
        "  for i in np.linspace(0.01, .99, num=100):\n",
        "    _, _, _, _, _, _, _, _, f1, _, _, _ = binary_stats(z, y, i, 0, 0)\n",
        "    data.append((i, f1))\n",
        "    if f1 > best_f1:\n",
        "      best_f1 = f1\n",
        "      best_t = i  \n",
        "  return best_t, data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvoEA-TU_QMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10 # We experimentally found no significant benefit after 10 epochs\n",
        "\n",
        "# For testing purposes only. Comment the following line out in typical usage. \n",
        "# EPOCHS = 1\n",
        "\n",
        "# Set some variables. \n",
        "FN_MARGINAL_COST = 2000 # E.g. if the disease is missed, it will get worse, costing an extra $2000 in future medical costs and pain and suffering\n",
        "FP_MARGINAL_COST = 100 # E.g. marginal cost of FP is the cost of running a second, more expensive test to identify the TN. \n",
        "\n",
        "stats = np.zeros((100,3,12))\n",
        "\n",
        "# Run it multiple times. \n",
        "for target in range (10):\n",
        "  for batch in range (10):\n",
        "    print(str(target) + \":\" + str(batch) + \":\" + str(datetime.datetime.now()))\n",
        "\n",
        "    #Processing on MNIST data\n",
        "    X = mnist.data.astype('float32')\n",
        "    y = mnist.target.astype('int64')\n",
        "    X /= 255.0\n",
        "\n",
        "    # Generate data. Return value is not randomized. \n",
        "    X, y = create_imbalanced_mnist_data(X, y, target, 630, batch)\n",
        "    # Shuffle and split into train and test data. \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
        "\n",
        "    # Build control and test models. \n",
        "    model_control = create_binary_model('binary_crossentropy')\n",
        "    loss = create_rww_binary_crossentropy(FN_MARGINAL_COST, FP_MARGINAL_COST)\n",
        "    model_test = create_binary_model(loss)\n",
        "\n",
        "    model_control.fit(x = X_train, y = y_train, epochs=EPOCHS, batch_size=100, validation_split=0.1, verbose=0)      \n",
        "    model_test.fit(x = X_train, y = y_train, epochs=EPOCHS, batch_size=100, validation_split=0.1, verbose=0)  \n",
        "\n",
        "    y_pred_c = model_control.predict(X_test)\n",
        "    y_pred_t = model_test.predict(X_test)\n",
        "\n",
        "    stats_control = binary_stats(y_pred_c, y_test, 0.5, FN_MARGINAL_COST, FP_MARGINAL_COST)\n",
        "    best_threshold, data = search_threshold(y_pred_c, y_test)\n",
        "    stats_control_adj = binary_stats(y_pred_c, y_test, best_threshold, FN_MARGINAL_COST, FP_MARGINAL_COST)\n",
        "    stats_test = binary_stats(y_pred_t, y_test, 0.5, FN_MARGINAL_COST, FP_MARGINAL_COST)\n",
        "\n",
        "    print(tabulate([('Control',) + stats_control, \\\n",
        "                    ('Control_adj',) + stats_control_adj, \\\n",
        "                    ('Experimental',) + stats_test], \\\n",
        "                   headers=['P', 'N', 'TP', \"FN\", \"TN\", \"FP\", \"Recall\", \\\n",
        "                            \"Precision\", \"F1\", \"Real World Cost\", \\\n",
        "                            \"Real World Benefit\", \"Err\"], \\\n",
        "                   floatfmt=\".6f\"))\n",
        "\n",
        "    plt.plot(np.array(data)[:,0], np.array(data)[:,1])\n",
        "    plt.title('F1 Score by Threshhold')\n",
        "\n",
        "    stats[target * 10 + batch, 0] = stats_control\n",
        "    stats[target * 10 + batch, 1] = stats_control_adj\n",
        "    stats[target * 10 + batch, 2] = stats_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-9wH-4bHFr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "control1_test_p_values = np.zeros((12,1))\n",
        "control2_test_p_values = np.zeros((12,1))\n",
        "\n",
        "for i in range(stats.shape[2]):\n",
        "  _, control1_test_p_values[i] = sp.stats.ttest_rel(stats[:, 0, i], stats[:, 2, i])\n",
        "  _, control2_test_p_values[i] = sp.stats.ttest_rel(stats[:, 1, i], stats[:, 2, i])\n",
        "\n",
        "print (\"n=\" + str(stats.shape[0]))\n",
        "  \n",
        "print(tabulate([('Control',) + tuple(np.mean(stats[:,0,:], axis=0)), \\\n",
        "                ('Control_adj',) + tuple(np.mean(stats[:,1,:], axis=0)), \\\n",
        "                ('Experimental',) + tuple(np.mean(stats[:,2,:], axis=0)), \\\n",
        "                ('control1_p_values',) + tuple(control1_test_p_values), \\\n",
        "                ('control2_p_values',) + tuple(control2_test_p_values) \\\n",
        "               ], \\\n",
        "               headers=['P', 'N', 'TP', \"FN\", \"TN\", \"FP\", \"Recall\", \\\n",
        "                        \"Precision\", \"F1\", \"Real World Cost\", \\\n",
        "                        \"Real World Benefit\", \"Err\"], \\\n",
        "               floatfmt=\".20f\"))\n",
        "\n",
        "print(tabulate([('Control',) + tuple(np.mean(stats[:,0,:], axis=0)), \\\n",
        "                ('Control_adj',) + tuple(np.mean(stats[:,1,:], axis=0)), \\\n",
        "                ('Experimental',) + tuple(np.mean(stats[:,2,:], axis=0)), \\\n",
        "                ('control1_p_values',) + tuple(control1_test_p_values), \\\n",
        "                ('control2_p_values',) + tuple(control2_test_p_values) \\\n",
        "               ], \\\n",
        "               headers=['P', 'N', 'TP', \"FN\", \"TN\", \"FP\", \"Recall\", \\\n",
        "                        \"Precision\", \"F1\", \"Real World Cost\", \\\n",
        "                        \"Real World Benefit\", \"Err\"], \\\n",
        "               floatfmt=\".20e\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHEB51REPE6U",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Graphical Display of Loss Function Mechanics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VASnHSpPPlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def J(z, y):\n",
        "  if (y==1):\n",
        "    return -np.log(z)\n",
        "  elif (y==0):\n",
        "    return -np.log(1-z)\n",
        "  else:\n",
        "    assert false\n",
        "\n",
        "def J_batch(p, n_h, n_t):\n",
        "  m = n_h + n_t\n",
        "  x = np.zeros(m)\n",
        "\n",
        "  for i in range(0,n_h):\n",
        "    x[i] = 1\n",
        "\n",
        "  retval = 0.\n",
        "  for i in range(0, m):\n",
        "    retval += J(p, x[i])\n",
        "  retval /= x.shape[0]\n",
        "  return retval\n",
        "\n",
        "x_and_y = []\n",
        "    \n",
        "for i in np.linspace(0.0001, 0.9999, 1000):\n",
        "  x_and_y.append((i, J_batch(i, 9, 1)))\n",
        "\n",
        "x_and_y = np.array(x_and_y)\n",
        "\n",
        "print(x_and_y.shape)\n",
        "  \n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(x_and_y[:,0], x_and_y[:,1])\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2EfTMy9YS1q",
        "colab_type": "text"
      },
      "source": [
        "# Bernoulli Trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWrXZk6VdYlW",
        "colab_type": "text"
      },
      "source": [
        "A quick Bernoulli trial. Bias should be close to 2.1. Sigmoid(2.1) = 0.9, which is the minimum cost / accurate prediction for 9 heads and 1 tail. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8faj_W8YcZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_y = k_x = Input(shape=(1,))\n",
        "k_y = Dense(1, activation='sigmoid', kernel_initializer='zeros',\n",
        "                bias_initializer='ones')(k_y)\n",
        "\n",
        "model_bernoulli = Model(inputs=k_x, outputs=k_y)\n",
        "model_bernoulli.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "X = np.zeros((10,))\n",
        "y = np.concatenate([np.ones((9,)), np.zeros((1,))])\n",
        "\n",
        "model_bernoulli.fit(x = X, y = y, epochs=1000, verbose=0) \n",
        "print(model_bernoulli.evaluate(x = X, y = y))\n",
        "print(model_bernoulli.get_weights()[1])\n",
        "\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "\n",
        "print(sigmoid(model_bernoulli.get_weights()[1][0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay07zsdvMw1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}