{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real-World-Weighted Crossentropy Loss Function",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaoshiang/The-Real-World-Weight-Crossentropy-Loss-Function/blob/master/Real_World_Weighted_Crossentropy_Loss_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y1dlLOBjGuy",
        "colab_type": "text"
      },
      "source": [
        "# Real-World-Weighted Crossentropy Loss Function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjW5dVQjjE6d",
        "colab_type": "text"
      },
      "source": [
        "This is a companion notebook to the paper published at <<<>>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOo8TfTRUIOX",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) 2019 Yaoshiang Ho\n",
        "\n",
        "This program is free software: you can redistribute it and/or modify\n",
        "it under the terms of the GNU General Public License as published by\n",
        "the Free Software Foundation, either version 3 of the License, or\n",
        "(at your option) any later version.\n",
        "\n",
        "This program is distributed in the hope that it will be useful,\n",
        "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "GNU General Public License for more details.\n",
        "\n",
        "Contact author for exceptions, which are intended to be widely granted. \n",
        "\n",
        "You should have received a copy of the GNU General Public License\n",
        "along with this program.  If not, see <https://www.gnu.org/licenses/>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6B-pD4hiHyi",
        "colab_type": "code",
        "outputId": "9c8c3d27-89d8-44bb-c8e4-31d7a9f67e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import set_random_seed\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import scipy as sp\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uUw19MXj_tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch MNIST dataset\n",
        "\n",
        "mnist = fetch_openml('mnist_784', cache=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6GkBRRwkHbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Processing on MNIST data\n",
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')\n",
        "X /= 255.0\n",
        "X = X\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRdr4rjpk0G2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(loss):\n",
        "  k_y = k_x = Input(shape=(784,))\n",
        "  k_y = Dense(50, activation='relu')(k_y)\n",
        "  k_y = Dense(20, activation='relu')(k_y)\n",
        "  k_y = Dense(10, activation='softmax')(k_y)\n",
        "\n",
        "  model = Model(inputs=k_x, outputs=k_y)\n",
        "  model.compile(optimizer='adam', loss=loss, metrics=['categorical_accuracy'])\n",
        "  return model\n",
        "\n",
        "# model_a = create_model(\"categorical_crossentropy\")\n",
        "# set_random_seed(1)\n",
        "# model_a.fit(x = X_train, y = to_categorical(y_train), epochs=10, batch_size=100, validation_split=0.1)\n",
        "# print(model_a.evaluate(X_test, to_categorical(y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI-1L6elsVe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_predictions(z, y):\n",
        "  # Inputs: \n",
        "  #   z: raw output of softmax for m classes\n",
        "  #   y: target predictions\n",
        "  # Outputs:\n",
        "  #   average predictions matrix of size (z.shape[1],z.shape[1])\n",
        "  #     axis 0 are the true categories\n",
        "  #     axis 1 are the predicted categories\n",
        "  #     each cell is the average of the predictions\n",
        "  m, n = z.shape\n",
        "  y_pred = np.argmax(z, axis=1)  \n",
        "  \n",
        "  averages = np.zeros((n, n))\n",
        "  \n",
        "  for i in range(0, n):\n",
        "    mask = y == np.array(i)\n",
        "    assert mask.shape == (m,)\n",
        "    np.mean(z[mask], axis=0, out=averages[i])\n",
        "    \n",
        "  return averages\n",
        "\n",
        "# print(\"Average predictions for model_a. Rows are y, columns are y_hat\")\n",
        "# print(average_predictions(model_a.predict(X_test), y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QovfFnw4oVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def num_predictions(z, y):\n",
        "  # Inputs: \n",
        "  #   z: raw output of softmax for m classes\n",
        "  #   y: target predictions using argmax\n",
        "  # Outputs:\n",
        "  #   total predictions matrix of size (z.shape[1],z.shape[1])\n",
        "  #     axis 0 are the true categories\n",
        "  #     axis 1 are the predicted categories\n",
        "  #     each cell is the sum of the predictions\n",
        "  m, n = z.shape\n",
        "  y_pred = np.argmax(z, axis=1)  \n",
        "  \n",
        "  sums = np.zeros((n, n))\n",
        "  \n",
        "  for i in range(0, n):\n",
        "    mask = y == np.array(i)\n",
        "    assert mask.shape == (m,)    \n",
        "    np.sum(to_categorical(np.argmax(z[mask], axis=1), num_classes=10), axis=0, out=sums[i])\n",
        "  return sums\n",
        "\n",
        "# print(\"Total number of predictions for model_a. Rows are y, columns are y_hat\")\n",
        "# np.set_printoptions(formatter={'float': lambda x: \"{0:6.0f}\".format(x)})\n",
        "# print(num_predictions(model_a.predict(X_test), y_test))\n",
        "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTJE_umwrIRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (C) 2019 Yaoshiang Ho\n",
        "\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "\n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "\n",
        "def create_rww_binary_crossentropy(fn_weight, fp_weight):\n",
        "  \"\"\"\n",
        "  binary_crossentropy is a special case of categorical_crossentropy.  \n",
        "  The \"all other\" column is imputed. For single-label, an implementation is provided.\n",
        "  However, this loss function is not very useful in a multi-label environment. \n",
        "  It only supports k=1. \n",
        "  \n",
        "  In future work, we will provide\n",
        "  an implementation for one target, one miscategorization. Note that this will\n",
        "  require a tensor of at lease size (k,k,2,2), where, the first 2 \n",
        "  represents T/F of target, the second 2 represents the FP/FN of label. \n",
        "\n",
        "  We may also create a fully expressive loss function, which would require some\n",
        "  efficient representation of a tensor of size (k, k, 2^k, 2^k)\n",
        "  \n",
        "  \"\"\"  \n",
        "  def binary_loss_function(target, output):\n",
        "    output = K.clip(output, K.epsilon(), 1 - K.epsilon()) \n",
        "    \n",
        "    logs = K.log(output) # shape (m, 1)\n",
        "    logs_1_sub = K.log(1-output) # shape (m, 1)\n",
        "\n",
        "    return - K.mean(target * fn_weight * logs + \n",
        "                    (1-target) * fp_weight * logs_1_sub)\n",
        "\n",
        "  return binary_loss_function\n",
        "\n",
        "def create_rww_categorical_crossentropy(k, loss_type, fn_weights=None, fp_weights=None, return_weights=False):\n",
        "  \"\"\"Real-World-Weighted crossentropy between an output tensor and a target tensor.\n",
        "  \n",
        "  The loss_types other than rww_categorical_crossentropy reimplement existing K/TF \n",
        "  functions but are not as optimized. \n",
        "  Those loss_types are useful when calling return_weights=True, which\n",
        "  then returns fn and fp weights of size (k,k). Editing those to reflect\n",
        "  real world costs, then passing them back into create_rww_crossentropy with \n",
        "  loss_type \"rww_crossentropy\" is the recommended appraoch. \n",
        "  \n",
        "  The fn and fp weights are easy to reason about. Suppose you have three classes: cat, dog, and other.\n",
        "  \n",
        "  Cat is one-hot encoded as [1,0,0] in a target. \n",
        "  \n",
        "  fn_weights of [x1, __, __]\n",
        "                [__, x2, __]\n",
        "                [__, __, x3]\n",
        " \n",
        "  fn_weights only applies to the one-hot in question. We dot to\n",
        "  \n",
        "  fn_weights of [x1, __, __]\n",
        "\n",
        "  x1 represents the scale of the cost for a fn, calculated as fn_weight * log(y_pred)\n",
        "\n",
        "  ...\n",
        "\n",
        "  fp_weights is concerned with the costs of the fps from the other classes. \n",
        "\n",
        "  fp_weights of [__, x1, x2]\n",
        "                [x3, __, x4]\n",
        "                [x5, x6, __]\n",
        " \n",
        "  fp_weights only applies to the zero labels. We again dot against [1, 0, 0]\n",
        "  \n",
        "  fp_weights of [__, x1, x2]\n",
        "\n",
        "  x1 represents the cost of predicting 1 for dog, when it should be zero. (log(1-y_pred))\n",
        "\n",
        "  \n",
        "  Args:\n",
        "    * k: 2 or more for single label, multiclass categorical crossentropy. \n",
        "      1 for the special case of binary_crossentropy\n",
        "    * loss_type: \"categorical_crossentropy\" to initialize to \n",
        "      standard softmax_crossentropy behavior, \n",
        "      or \"weighted_categorical_crossentropy\" for standard behavior, or, \n",
        "      or \"rww_crossentropy\" for full weight matrix of all possible fn/fp combinations. \n",
        "      \"binary_crossentropy\" is available but is not useful in multi-label\n",
        "      environments. It is only useful for classic binary logistic regression. \n",
        "      k must be 2 for binary_crossentropy. fp_weights and fn_weights should\n",
        "      each be scalars. \n",
        "    * fn_weights: a numpy array of \n",
        "      shape (k) for standard tf.nn.weighted_crossentropy behavior focusing on \n",
        "      false negatives. This will internally be converted to a matrix of (k,k)\n",
        "      along only the main diagonal by multiplying by identity. \n",
        "    * fp_weights: a numpy array of shape (k,k) to define specific combinations \n",
        "      of false positive. The main diag should be zeros. \n",
        "    * return_weights: If False (default), returns cost function. If True, \n",
        "      returns fn and fp weights as np.array. Useful if changes\n",
        "      necessary before passing back to create_rww_crossentropy. \n",
        "Returns:\n",
        "    * retval: Loss function for use Keras.model.fit, or if return_weights\n",
        "      arg is True, the fn_weights and fp_weights. \n",
        "  \"\"\"\n",
        "\n",
        "  full_fn_weights = None\n",
        "  full_fp_weights = None\n",
        "\n",
        "  anti_eye = np.ones((k,k)) - np.eye(k)\n",
        "    \n",
        "  if (loss_type==\"categorical_crossentropy\"):\n",
        "    full_fn_weights = np.identity((k))\n",
        "    full_fp_weights = np.zeros((k, k)) # Softmax crossentropy ignores fp.\n",
        "\n",
        "  elif(loss_type==\"weighted_categorical_crossentropy\"):\n",
        "    full_fn_weights = np.eye(k) * fn_weights\n",
        "    full_fp_weights = np.zeros((k, k)) # softmax crossentropy ignores fp\n",
        "    \n",
        "  elif(loss_type==\"rww_crossentropy\"):\n",
        "    assert not np.count_nonzero(fn_weights * anti_eye)\n",
        "    assert not np.count_nonzero(fp_weights * np.eye(k))\n",
        "\n",
        "    full_fn_weights = fn_weights\n",
        "    # Novel piece: allow any combination of fp.\n",
        "    full_fp_weights = fp_weights\n",
        "    \n",
        "  else:\n",
        "    raise Exception(\"unknown loss_type: \" + str(loss_type))\n",
        "   \n",
        "  fn_wt = K.constant(full_fn_weights) # (k,k), always sparse along main diag. \n",
        "  fp_wt = K.constant(full_fp_weights) # (k,k), always dense except main diag. \n",
        "\n",
        "  def loss_function(target, output):\n",
        "    output = K.clip(output, K.epsilon(), 1 - K.epsilon()) \n",
        "    \n",
        "    logs = K.log(output) # shape (m, k), dense. 1 is good. \n",
        "    logs_1_sub = K.log(1-output) # shape (m, k), dense. 0 is good. \n",
        "\n",
        "    m_full_fn_weights = K.dot(target, fn_wt) # (m,k) . (k, k)\n",
        "    m_full_fp_weights = K.dot(target, fp_wt) # (m,k) . (k, k)\n",
        "\n",
        "    return - K.mean(m_full_fn_weights * logs + \n",
        "                    m_full_fp_weights * logs_1_sub)\n",
        "  \n",
        "  if (return_weights):\n",
        "    return full_fn_weights, full_fp_weights\n",
        "  else:\n",
        "    return loss_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8uIO9axuDkW",
        "colab_type": "code",
        "outputId": "20b74675-aa80-4482-f006-5a8024e39378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        }
      },
      "source": [
        "# Example of categorical_crossentropy. Class behavior. \n",
        "fn_weights, fp_weights = create_rww_categorical_crossentropy(5, 'categorical_crossentropy', return_weights=True)\n",
        "print(\"softmax cross entropy weights. k=5 classes. Expected output: fn: ones, fp:zeros. \")\n",
        "print (fn_weights)\n",
        "print (fp_weights)\n",
        "\n",
        "# Example of weighted single-label categorical crossentropy. \n",
        "fn_weights, fp_weights = create_rww_categorical_crossentropy(5, 'weighted_categorical_crossentropy', np.array([1,2,3,4,5]), return_weights=True)\n",
        "print(\"weighted softmax crossentropy. k=5 classes. Expected output: fn: weights, fp: zeros. \")\n",
        "print (fn_weights)\n",
        "print (fp_weights)\n",
        "\n",
        "\n",
        "# Example of full weighted multi-label categorical crossentropy\n",
        "fn_weights, fp_weights = create_rww_categorical_crossentropy(10, \"categorical_crossentropy\", return_weights=True)\n",
        "fp_weights[5, 8] = 19\n",
        "loss = create_rww_categorical_crossentropy(10, \"rww_crossentropy\", fn_weights, fp_weights)\n",
        "print(\"real world weighted cross entropy weights. k=10 classes. If target label if 5, heavily penalize a (false positive) 8. \")\n",
        "print (fn_weights)\n",
        "print (fp_weights)\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax cross entropy weights. k=5 classes. Expected output: fn: ones, fp:zeros. \n",
            "[[1.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 1.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 1.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 1.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 1.00]]\n",
            "[[0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]]\n",
            "weighted softmax crossentropy. k=5 classes. Expected output: fn: weights, fp: zeros. \n",
            "[[1.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 2.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 3.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 4.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 5.00]]\n",
            "[[0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00]]\n",
            "real world weighted cross entropy weights. k=10 classes. If target label if 5, heavily penalize a (false positive) 8. \n",
            "[[1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00]]\n",
            "[[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 19.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
            " [0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]]\n",
            "<function create_rww_categorical_crossentropy.<locals>.loss_function at 0x7f27124e8bf8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuwzaTB3n75K",
        "colab_type": "code",
        "outputId": "d38ee286-ce6c-4f8c-f08c-d273a9c64341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def generate_unequal_pair(i):\n",
        "  np.random.seed(i)\n",
        "  a = np.random.randint(0, 10)\n",
        "  b = np.random.randint(0,10)\n",
        "  while a == b:\n",
        "    b = np.random.randint(0,10)\n",
        "  return a, b\n",
        "\n",
        "# Repeatability\n",
        "np.random.seed(1)\n",
        "set_random_seed(1)\n",
        "\n",
        "# Quick compare 10 sample and 10 control networks\n",
        "for i in range(0,10):\n",
        "  print(\".\", end = \" \")   \n",
        "  # Generate random socially biased or expensive fn,fp pair, setup rww_crossentropy loss function. \n",
        "  on_label, offensive_fp = generate_unequal_pair(i)  \n",
        "  fn_weights, fp_weights = create_rww_categorical_crossentropy(10, \"categorical_crossentropy\", return_weights=True)\n",
        "  fp_weights[on_label, offensive_fp] = 19\n",
        "  loss = create_rww_categorical_crossentropy(10, \"rww_crossentropy\", fn_weights, fp_weights)\n",
        "\n",
        "  # Train control model\n",
        "  model_control = create_model('binary_crossentropy') # K/TF builtin\n",
        "  model_control.fit(x = X_train, y = to_categorical(y_train), epochs=10, batch_size=100, validation_split=0.1, verbose=0)\n",
        "  num_pred_control = num_predictions(model_control.predict(X_test), y_test)\n",
        "  _, acc_control = model_control.evaluate(X_test, to_categorical(y_test))\n",
        "  \n",
        "  # Train experimental model with same epochs / batchsize as control model. \n",
        "  model_test = create_model(loss) # RWW version\n",
        "  model_test.fit(x = X_train, y = to_categorical(y_train), epochs=10, batch_size=100, validation_split=0.1, verbose=0)  \n",
        "  num_pred_test = num_predictions(model_test.predict(X_test), y_test)\n",
        "  _, acc_test = model_test.evaluate(X_test, to_categorical(y_test), verbose=0)\n",
        "\n",
        "  print(\"For test (fn:fp):(\" + str(on_label) + \",\" + \\\n",
        "        str(offensive_fp) + \"): FPs from (control, experimental): (\" + \\\n",
        "        str(num_pred_control[on_label, offensive_fp]) + \",\" +\\\n",
        "        str(num_pred_test[on_label, offensive_fp]) + \\\n",
        "        \"); Accuracy from (control, experimental):\" + \\\n",
        "        '{:0.4f}'.format(acc_control) + \",\", '{:0.4f}'.format(acc_test) + \")\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ". Train on 47250 samples, validate on 5250 samples\n",
            "Epoch 1/10\n",
            "47250/47250 [==============================] - 3s 64us/step - loss: 0.0745 - categorical_accuracy: 0.8695 - val_loss: 0.0421 - val_categorical_accuracy: 0.9270\n",
            "Epoch 2/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0349 - categorical_accuracy: 0.9401 - val_loss: 0.0335 - val_categorical_accuracy: 0.9451\n",
            "Epoch 3/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0277 - categorical_accuracy: 0.9539 - val_loss: 0.0288 - val_categorical_accuracy: 0.9510\n",
            "Epoch 4/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0237 - categorical_accuracy: 0.9607 - val_loss: 0.0259 - val_categorical_accuracy: 0.9571\n",
            "Epoch 5/10\n",
            "47250/47250 [==============================] - 3s 55us/step - loss: 0.0204 - categorical_accuracy: 0.9658 - val_loss: 0.0249 - val_categorical_accuracy: 0.9592\n",
            "Epoch 6/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0181 - categorical_accuracy: 0.9704 - val_loss: 0.0230 - val_categorical_accuracy: 0.9646\n",
            "Epoch 7/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0159 - categorical_accuracy: 0.9738 - val_loss: 0.0227 - val_categorical_accuracy: 0.9621\n",
            "Epoch 8/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0144 - categorical_accuracy: 0.9765 - val_loss: 0.0210 - val_categorical_accuracy: 0.9674\n",
            "Epoch 9/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0132 - categorical_accuracy: 0.9785 - val_loss: 0.0232 - val_categorical_accuracy: 0.9640\n",
            "Epoch 10/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0119 - categorical_accuracy: 0.9802 - val_loss: 0.0217 - val_categorical_accuracy: 0.9659\n",
            "17500/17500 [==============================] - 1s 56us/step\n",
            "Train on 47250 samples, validate on 5250 samples\n",
            "Epoch 1/10\n",
            "47250/47250 [==============================] - 3s 54us/step - loss: 0.0557 - categorical_accuracy: 0.8430 - val_loss: 0.0296 - val_categorical_accuracy: 0.9211\n",
            "Epoch 2/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0230 - categorical_accuracy: 0.9350 - val_loss: 0.0226 - val_categorical_accuracy: 0.9398\n",
            "Epoch 3/10\n",
            "47250/47250 [==============================] - 2s 46us/step - loss: 0.0178 - categorical_accuracy: 0.9487 - val_loss: 0.0192 - val_categorical_accuracy: 0.9501\n",
            "Epoch 4/10\n",
            "47250/47250 [==============================] - 2s 46us/step - loss: 0.0151 - categorical_accuracy: 0.9565 - val_loss: 0.0177 - val_categorical_accuracy: 0.9530\n",
            "Epoch 5/10\n",
            "47250/47250 [==============================] - 2s 46us/step - loss: 0.0133 - categorical_accuracy: 0.9618 - val_loss: 0.0162 - val_categorical_accuracy: 0.9592\n",
            "Epoch 6/10\n",
            "47250/47250 [==============================] - 2s 46us/step - loss: 0.0115 - categorical_accuracy: 0.9670 - val_loss: 0.0161 - val_categorical_accuracy: 0.9571\n",
            "Epoch 7/10\n",
            "47250/47250 [==============================] - 2s 48us/step - loss: 0.0102 - categorical_accuracy: 0.9696 - val_loss: 0.0168 - val_categorical_accuracy: 0.9592\n",
            "Epoch 8/10\n",
            "47250/47250 [==============================] - 2s 49us/step - loss: 0.0092 - categorical_accuracy: 0.9729 - val_loss: 0.0144 - val_categorical_accuracy: 0.9659\n",
            "Epoch 9/10\n",
            "47250/47250 [==============================] - 2s 48us/step - loss: 0.0085 - categorical_accuracy: 0.9750 - val_loss: 0.0143 - val_categorical_accuracy: 0.9615\n",
            "Epoch 10/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0076 - categorical_accuracy: 0.9776 - val_loss: 0.0146 - val_categorical_accuracy: 0.9648\n",
            "For test (fn:fp):(5,0): FPs from (control, experimental): (1.0,1.0); Accuracy from (control, experimental):0.9640, 0.9614)\n",
            ". Train on 47250 samples, validate on 5250 samples\n",
            "Epoch 1/10\n",
            "47250/47250 [==============================] - 3s 70us/step - loss: 0.0794 - categorical_accuracy: 0.8568 - val_loss: 0.0457 - val_categorical_accuracy: 0.9244\n",
            "Epoch 2/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0368 - categorical_accuracy: 0.9373 - val_loss: 0.0361 - val_categorical_accuracy: 0.9390\n",
            "Epoch 3/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0288 - categorical_accuracy: 0.9519 - val_loss: 0.0299 - val_categorical_accuracy: 0.9505\n",
            "Epoch 4/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0242 - categorical_accuracy: 0.9603 - val_loss: 0.0270 - val_categorical_accuracy: 0.9541\n",
            "Epoch 5/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0209 - categorical_accuracy: 0.9660 - val_loss: 0.0258 - val_categorical_accuracy: 0.9571\n",
            "Epoch 6/10\n",
            "47250/47250 [==============================] - 3s 58us/step - loss: 0.0185 - categorical_accuracy: 0.9692 - val_loss: 0.0249 - val_categorical_accuracy: 0.9596\n",
            "Epoch 7/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0164 - categorical_accuracy: 0.9735 - val_loss: 0.0230 - val_categorical_accuracy: 0.9632\n",
            "Epoch 8/10\n",
            "47250/47250 [==============================] - 3s 55us/step - loss: 0.0148 - categorical_accuracy: 0.9758 - val_loss: 0.0214 - val_categorical_accuracy: 0.9659\n",
            "Epoch 9/10\n",
            "47250/47250 [==============================] - 3s 55us/step - loss: 0.0134 - categorical_accuracy: 0.9786 - val_loss: 0.0218 - val_categorical_accuracy: 0.9663\n",
            "Epoch 10/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0120 - categorical_accuracy: 0.9806 - val_loss: 0.0204 - val_categorical_accuracy: 0.9678\n",
            "17500/17500 [==============================] - 1s 55us/step\n",
            "Train on 47250 samples, validate on 5250 samples\n",
            "Epoch 1/10\n",
            "47250/47250 [==============================] - 3s 55us/step - loss: 0.0550 - categorical_accuracy: 0.8539 - val_loss: 0.0294 - val_categorical_accuracy: 0.9202\n",
            "Epoch 2/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0240 - categorical_accuracy: 0.9343 - val_loss: 0.0254 - val_categorical_accuracy: 0.9383\n",
            "Epoch 3/10\n",
            "47250/47250 [==============================] - 2s 46us/step - loss: 0.0183 - categorical_accuracy: 0.9488 - val_loss: 0.0191 - val_categorical_accuracy: 0.9484\n",
            "Epoch 4/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0154 - categorical_accuracy: 0.9560 - val_loss: 0.0172 - val_categorical_accuracy: 0.9524\n",
            "Epoch 5/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0133 - categorical_accuracy: 0.9621 - val_loss: 0.0158 - val_categorical_accuracy: 0.9579\n",
            "Epoch 6/10\n",
            "47250/47250 [==============================] - 2s 48us/step - loss: 0.0112 - categorical_accuracy: 0.9675 - val_loss: 0.0162 - val_categorical_accuracy: 0.9619\n",
            "Epoch 7/10\n",
            "47250/47250 [==============================] - 2s 46us/step - loss: 0.0103 - categorical_accuracy: 0.9701 - val_loss: 0.0154 - val_categorical_accuracy: 0.9613\n",
            "Epoch 8/10\n",
            "47250/47250 [==============================] - 2s 46us/step - loss: 0.0091 - categorical_accuracy: 0.9735 - val_loss: 0.0152 - val_categorical_accuracy: 0.9644\n",
            "Epoch 9/10\n",
            "47250/47250 [==============================] - 2s 48us/step - loss: 0.0088 - categorical_accuracy: 0.9746 - val_loss: 0.0138 - val_categorical_accuracy: 0.9634\n",
            "Epoch 10/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0074 - categorical_accuracy: 0.9783 - val_loss: 0.0142 - val_categorical_accuracy: 0.9606\n",
            "For test (fn:fp):(5,8): FPs from (control, experimental): (7.0,3.0); Accuracy from (control, experimental):0.9646, 0.9562)\n",
            ". Train on 47250 samples, validate on 5250 samples\n",
            "Epoch 1/10\n",
            "47250/47250 [==============================] - 3s 66us/step - loss: 0.0757 - categorical_accuracy: 0.8694 - val_loss: 0.0442 - val_categorical_accuracy: 0.9291\n",
            "Epoch 2/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0358 - categorical_accuracy: 0.9391 - val_loss: 0.0348 - val_categorical_accuracy: 0.9450\n",
            "Epoch 3/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0282 - categorical_accuracy: 0.9531 - val_loss: 0.0288 - val_categorical_accuracy: 0.9528\n",
            "Epoch 4/10\n",
            "47250/47250 [==============================] - 3s 53us/step - loss: 0.0232 - categorical_accuracy: 0.9616 - val_loss: 0.0271 - val_categorical_accuracy: 0.9573\n",
            "Epoch 5/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0198 - categorical_accuracy: 0.9674 - val_loss: 0.0240 - val_categorical_accuracy: 0.9610\n",
            "Epoch 6/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0172 - categorical_accuracy: 0.9719 - val_loss: 0.0222 - val_categorical_accuracy: 0.9657\n",
            "Epoch 7/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0148 - categorical_accuracy: 0.9760 - val_loss: 0.0211 - val_categorical_accuracy: 0.9657\n",
            "Epoch 8/10\n",
            "47250/47250 [==============================] - 3s 55us/step - loss: 0.0132 - categorical_accuracy: 0.9786 - val_loss: 0.0205 - val_categorical_accuracy: 0.9661\n",
            "Epoch 9/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0119 - categorical_accuracy: 0.9806 - val_loss: 0.0199 - val_categorical_accuracy: 0.9676\n",
            "Epoch 10/10\n",
            "47250/47250 [==============================] - 3s 58us/step - loss: 0.0104 - categorical_accuracy: 0.9830 - val_loss: 0.0201 - val_categorical_accuracy: 0.9693\n",
            "17500/17500 [==============================] - 1s 60us/step\n",
            "Train on 47250 samples, validate on 5250 samples\n",
            "Epoch 1/10\n",
            "47250/47250 [==============================] - 3s 62us/step - loss: 0.0517 - categorical_accuracy: 0.8541 - val_loss: 0.0351 - val_categorical_accuracy: 0.9023\n",
            "Epoch 2/10\n",
            "47250/47250 [==============================] - 2s 50us/step - loss: 0.0229 - categorical_accuracy: 0.9364 - val_loss: 0.0230 - val_categorical_accuracy: 0.9387\n",
            "Epoch 3/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0179 - categorical_accuracy: 0.9504 - val_loss: 0.0195 - val_categorical_accuracy: 0.9501\n",
            "Epoch 4/10\n",
            "47250/47250 [==============================] - 2s 48us/step - loss: 0.0150 - categorical_accuracy: 0.9581 - val_loss: 0.0167 - val_categorical_accuracy: 0.9541\n",
            "Epoch 5/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0129 - categorical_accuracy: 0.9639 - val_loss: 0.0171 - val_categorical_accuracy: 0.9512\n",
            "Epoch 6/10\n",
            "47250/47250 [==============================] - 2s 49us/step - loss: 0.0113 - categorical_accuracy: 0.9677 - val_loss: 0.0146 - val_categorical_accuracy: 0.9579\n",
            "Epoch 7/10\n",
            "47250/47250 [==============================] - 2s 48us/step - loss: 0.0101 - categorical_accuracy: 0.9715 - val_loss: 0.0145 - val_categorical_accuracy: 0.9598\n",
            "Epoch 8/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0090 - categorical_accuracy: 0.9745 - val_loss: 0.0127 - val_categorical_accuracy: 0.9629\n",
            "Epoch 9/10\n",
            "47250/47250 [==============================] - 2s 48us/step - loss: 0.0080 - categorical_accuracy: 0.9767 - val_loss: 0.0127 - val_categorical_accuracy: 0.9642\n",
            "Epoch 10/10\n",
            "47250/47250 [==============================] - 2s 47us/step - loss: 0.0073 - categorical_accuracy: 0.9792 - val_loss: 0.0126 - val_categorical_accuracy: 0.9646\n",
            "For test (fn:fp):(8,6): FPs from (control, experimental): (7.0,2.0); Accuracy from (control, experimental):0.9653, 0.9622)\n",
            ". Train on 47250 samples, validate on 5250 samples\n",
            "Epoch 1/10\n",
            "47250/47250 [==============================] - 3s 69us/step - loss: 0.0786 - categorical_accuracy: 0.8634 - val_loss: 0.0439 - val_categorical_accuracy: 0.9301\n",
            "Epoch 2/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0361 - categorical_accuracy: 0.9380 - val_loss: 0.0337 - val_categorical_accuracy: 0.9434\n",
            "Epoch 3/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0278 - categorical_accuracy: 0.9531 - val_loss: 0.0287 - val_categorical_accuracy: 0.9524\n",
            "Epoch 4/10\n",
            "47250/47250 [==============================] - 3s 55us/step - loss: 0.0232 - categorical_accuracy: 0.9613 - val_loss: 0.0259 - val_categorical_accuracy: 0.9585\n",
            "Epoch 5/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0196 - categorical_accuracy: 0.9681 - val_loss: 0.0241 - val_categorical_accuracy: 0.9602\n",
            "Epoch 6/10\n",
            "47250/47250 [==============================] - 3s 55us/step - loss: 0.0174 - categorical_accuracy: 0.9708 - val_loss: 0.0240 - val_categorical_accuracy: 0.9589\n",
            "Epoch 7/10\n",
            "47250/47250 [==============================] - 2s 53us/step - loss: 0.0154 - categorical_accuracy: 0.9744 - val_loss: 0.0225 - val_categorical_accuracy: 0.9625\n",
            "Epoch 8/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0139 - categorical_accuracy: 0.9775 - val_loss: 0.0225 - val_categorical_accuracy: 0.9648\n",
            "Epoch 9/10\n",
            "47250/47250 [==============================] - 3s 56us/step - loss: 0.0126 - categorical_accuracy: 0.9794 - val_loss: 0.0211 - val_categorical_accuracy: 0.9686\n",
            "Epoch 10/10\n",
            "47250/47250 [==============================] - 3s 57us/step - loss: 0.0114 - categorical_accuracy: 0.9815 - val_loss: 0.0204 - val_categorical_accuracy: 0.9691\n",
            "17500/17500 [==============================] - 1s 56us/step\n",
            "Train on 47250 samples, validate on 5250 samples\n",
            "Epoch 1/10\n",
            "47250/47250 [==============================] - 3s 62us/step - loss: 0.0538 - categorical_accuracy: 0.8509 - val_loss: 0.0289 - val_categorical_accuracy: 0.9238\n",
            "Epoch 2/10\n",
            "25700/47250 [===============>..............] - ETA: 1s - loss: 0.0240 - categorical_accuracy: 0.9346"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8c76f7569d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# Train experimental model with same epochs / batchsize as control model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mmodel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# RWW version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mnum_pred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BguZppn4sMfJ",
        "colab_type": "code",
        "outputId": "90231568-dc42-4846-a9cf-3ade6c700158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Train 90 control and 90 experimental models. \n",
        "\n",
        "# Repeatability\n",
        "np.random.seed(1)\n",
        "set_random_seed(1)\n",
        "\n",
        "accs = []\n",
        "fps = []\n",
        "preds = []\n",
        "ws = []\n",
        "\n",
        "for i in range(0,10):\n",
        "  for j in range (0,10):\n",
        "    if i == j: \n",
        "      continue\n",
        "\n",
        "    print(\".\", end = \" \")   \n",
        "    fn_weights, fp_weights = create_rww_categorical_crossentropy(10, \"categorical_crossentropy\", return_weights=True)\n",
        "    fp_weights[i, j] = 19\n",
        "    loss = create_rww_categorical_crossentropy(10, \"rww_crossentropy\", fn_weights, fp_weights)\n",
        "\n",
        "    # Train control model\n",
        "    model_control = create_model('binary_crossentropy') # K/TF builtin\n",
        "    model_control.fit(x = X_train, y = to_categorical(y_train), epochs=10, batch_size=100, validation_split=0.1, verbose=0)\n",
        "    num_pred_control = num_predictions(model_control.predict(X_test, verbose=0), y_test)\n",
        "    _, acc_control = model_control.evaluate(X_test, to_categorical(y_test), verbose=0)\n",
        " \n",
        "    # Train experimental model with same epochs / batchsize as control model. \n",
        "    model_test = create_model(loss) # RWW version\n",
        "    model_test.fit(x = X_train, y = to_categorical(y_train), epochs=10, batch_size=100, validation_split=0.1, verbose=0)  \n",
        "    num_pred_test = num_predictions(model_test.predict(X_test, verbose=0), y_test)\n",
        "    _, acc_test = model_test.evaluate(X_test, to_categorical(y_test), verbose=0)\n",
        "\n",
        "    accs.append((acc_control, acc_test))\n",
        "    fps.append((num_pred_control[i, j], num_pred_test[i,j]))\n",
        "    preds.append((num_pred_control, num_pred_test))\n",
        "    ws.append((fn_weights, fp_weights))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPb0t2YA-xs7",
        "colab_type": "code",
        "outputId": "6f5a97c2-07f5-41d0-d728-ce14f4429d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(fps.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALM2izF7lQVs",
        "colab_type": "code",
        "outputId": "b06c3661-4906-428c-9a68-e4f45314ea92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Perform t-test on results (number of mislabels). We use Welch's test since the variances are different. \n",
        "\n",
        "fps = np.array(fps)\n",
        "statistic, pvalue = sp.stats.ttest_ind(fps[:,0], fps[:,1], equal_var=False)\n",
        "\n",
        "print(np.mean(fps[:,0]))\n",
        "print(np.mean(fps[:,1]))\n",
        "\n",
        "print(statistic)\n",
        "print(pvalue)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.511111111111111\n",
            "2.533333333333333\n",
            "5.739868646596386\n",
            "6.979155459424445e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIP-u94mpcWZ",
        "colab_type": "code",
        "outputId": "d44b3327-c81a-457b-b098-e0cb0aaeae69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Perform t-test on errors\n",
        "\n",
        "accs = np.array(accs)\n",
        "statistic, pvalue = sp.stats.ttest_ind(1. - accs[:,0], 1. - accs[:,1])\n",
        "\n",
        "print(np.mean(1. - accs[:,0]))\n",
        "print(np.mean(1. - accs[:,1]))\n",
        "\n",
        "print(statistic)\n",
        "print(pvalue)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.03562285713771032\n",
            "0.03624444443278843\n",
            "-2.232330137956822\n",
            "0.02684018059521198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWEL1WvxXMny",
        "colab_type": "code",
        "outputId": "a0fc3cc8-53eb-4007-c05c-a9a7e4a0615e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Calculate Real World Cost\n",
        "def clip_correct(preds):\n",
        "  k = preds.shape[0]\n",
        "  assert k == preds.shape[1]\n",
        "  \n",
        "  anti_eye = 1 - np.eye(k)\n",
        "  return preds * anti_eye # k,k, with the main diagonal masked out to zeros\n",
        "\n",
        "def categorical_real_world_cost(preds, fn_ws, fp_ws):\n",
        "  k = preds.shape[0]\n",
        "  assert k == preds.shape[1]\n",
        "  \n",
        "  false_preds = clip_correct(preds)\n",
        "  fn_cost = np.sum(np.sum(false_preds, axis=-1) * np.sum(fn_ws, axis=-1)) \n",
        "  fp_cost = np.sum(false_preds * fp_ws)\n",
        " \n",
        "  return (fn_cost + fp_cost) / np.sum(preds)\n",
        "\n",
        "def vector_categorical_real_world_cost(m, preds, fn_ws, fp_ws):\n",
        "  retval = np.zeros(m)\n",
        "  for i in range(0,m):\n",
        "    retval[i] = categorical_real_world_cost(preds[i], fn_ws[i], fp_ws[i])\n",
        "  return retval\n",
        "    \n",
        "fn_ws = np.array(ws)[:,0,:,:] # size (90, k, k)\n",
        "fp_ws = np.array(ws)[:,1,:,:] # size (90, k, k)\n",
        "\n",
        "preds_c = np.array(preds)[:,0,:,:] # size (90, k, k)\n",
        "preds_t = np.array(preds)[:,1,:,:] # size (90, k, k)\n",
        "\n",
        "costs_c = vector_categorical_real_world_cost(90, preds_c, fn_ws, fp_ws)\n",
        "costs_t = vector_categorical_real_world_cost(90, preds_t, fn_ws, fp_ws)\n",
        "\n",
        "assert costs_c.shape == (90,)\n",
        "assert costs_t.shape == (90,)\n",
        "\n",
        "print(np.mean(costs_c))\n",
        "print(np.mean(costs_t))\n",
        "\n",
        "statistic, pvalue = sp.stats.ttest_ind(costs_c, costs_t)\n",
        "print(statistic)\n",
        "print(pvalue)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.04269206349206349\n",
            "0.03899492063492063\n",
            "4.46785326959378\n",
            "1.4015370329833121e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pko_2OWPsnIG",
        "colab_type": "text"
      },
      "source": [
        "# Testing against imbalanced binary data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTjERIGrBzP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch MNIST dataset\n",
        "\n",
        "# mnist = fetch_openml('mnist_784', cache=False)\n",
        "\n",
        "#Processing on MNIST data\n",
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')\n",
        "X /= 255.0\n",
        "\n",
        "def create_imbalanced_mnist_data(X, y, target, num_minority):\n",
        "  new_X, new_y = [], []\n",
        "  \n",
        "  mask = y == target\n",
        "  anti_mask = y != target\n",
        "\n",
        "  X_label = X[mask]  \n",
        "  y_label = y[mask]\n",
        "    \n",
        "  X_label = X[mask]\n",
        "  np.random.seed(1)\n",
        "  np.random.shuffle(new_X)\n",
        "  X_label = X_label[0:num_minority,:]\n",
        "  \n",
        "  new_X = np.concatenate([X_label, X[anti_mask]])\n",
        "  new_y = np.concatenate([np.ones((num_minority)), np.zeros(X[anti_mask].shape[0])])\n",
        "  \n",
        "  return new_X, new_y\n",
        "  \n",
        "X, y = create_imbalanced_mnist_data(X, y, 5, 700)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ0fRWhdzPju",
        "colab_type": "code",
        "outputId": "e596d780-5601-44dc-9439-a4316791d679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def create_binary_model(loss):\n",
        "  np.random.seed(1)\n",
        "  set_random_seed(1)\n",
        "\n",
        "  k_y = k_x = Input(shape=(784,))\n",
        "  # k_y = Dense(100, activation='relu')(k_y)\n",
        "  k_y = Dense(50, activation='relu')(k_y)\n",
        "  k_y = Dense(10, activation='relu')(k_y)\n",
        "  k_y = Dense(1, activation='sigmoid')(k_y)\n",
        "\n",
        "  model = Model(inputs=k_x, outputs=k_y)\n",
        "  model.compile(optimizer='adam', loss=loss, metrics=['binary_accuracy'])\n",
        "  return model\n",
        "  \n",
        "FN_MARGINAL_COST = 2000 # E.g. if the disease is missed, it will get worse, costing an extra $2000 in future medical costs and pain and suffering\n",
        "FP_MARGINAL_COST = 100 # E.g. marginal cost of FP is the cost of running a second, more expensive test to identify the TN. \n",
        "\n",
        "model_control = create_binary_model('binary_crossentropy')\n",
        "\n",
        "loss = create_rww_binary_crossentropy(FN_MARGINAL_COST, FP_MARGINAL_COST)\n",
        "model_test = create_binary_model(loss)\n",
        "\n",
        "model_control.fit(x = X_train, y = y_train, epochs=10, batch_size=100, validation_split=0.1, verbose=1)  \n",
        "model_test.fit(x = X_train, y = y_train, epochs=10, batch_size=100, validation_split=0.1, verbose=1)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0904 14:34:32.525773 140544463857536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0904 14:34:32.527151 140544463857536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0904 14:34:32.536085 140544463857536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0904 14:34:32.582772 140544463857536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0904 14:34:32.606411 140544463857536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0904 14:34:32.612926 140544463857536 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0904 14:34:32.924853 140544463857536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 43461 samples, validate on 4829 samples\n",
            "Epoch 1/10\n",
            "43461/43461 [==============================] - 7s 156us/step - loss: 0.0467 - binary_accuracy: 0.9901 - val_loss: 0.0190 - val_binary_accuracy: 0.9954\n",
            "Epoch 2/10\n",
            "43461/43461 [==============================] - 2s 52us/step - loss: 0.0179 - binary_accuracy: 0.9949 - val_loss: 0.0128 - val_binary_accuracy: 0.9959\n",
            "Epoch 3/10\n",
            "43461/43461 [==============================] - 2s 52us/step - loss: 0.0125 - binary_accuracy: 0.9964 - val_loss: 0.0111 - val_binary_accuracy: 0.9969\n",
            "Epoch 4/10\n",
            "43461/43461 [==============================] - 2s 50us/step - loss: 0.0092 - binary_accuracy: 0.9971 - val_loss: 0.0087 - val_binary_accuracy: 0.9973\n",
            "Epoch 5/10\n",
            "43461/43461 [==============================] - 2s 51us/step - loss: 0.0072 - binary_accuracy: 0.9980 - val_loss: 0.0087 - val_binary_accuracy: 0.9975\n",
            "Epoch 6/10\n",
            "43461/43461 [==============================] - 2s 51us/step - loss: 0.0056 - binary_accuracy: 0.9983 - val_loss: 0.0085 - val_binary_accuracy: 0.9971\n",
            "Epoch 7/10\n",
            "43461/43461 [==============================] - 2s 50us/step - loss: 0.0047 - binary_accuracy: 0.9985 - val_loss: 0.0085 - val_binary_accuracy: 0.9971\n",
            "Epoch 8/10\n",
            "43461/43461 [==============================] - 2s 51us/step - loss: 0.0036 - binary_accuracy: 0.9987 - val_loss: 0.0068 - val_binary_accuracy: 0.9973\n",
            "Epoch 9/10\n",
            "43461/43461 [==============================] - 2s 50us/step - loss: 0.0027 - binary_accuracy: 0.9992 - val_loss: 0.0071 - val_binary_accuracy: 0.9979\n",
            "Epoch 10/10\n",
            "43461/43461 [==============================] - 2s 51us/step - loss: 0.0020 - binary_accuracy: 0.9994 - val_loss: 0.0069 - val_binary_accuracy: 0.9979\n",
            "Train on 43461 samples, validate on 4829 samples\n",
            "Epoch 1/10\n",
            "43461/43461 [==============================] - 2s 46us/step - loss: 22.1240 - binary_accuracy: 0.9809 - val_loss: 9.8240 - val_binary_accuracy: 0.9944\n",
            "Epoch 2/10\n",
            "43461/43461 [==============================] - 2s 42us/step - loss: 8.2935 - binary_accuracy: 0.9885 - val_loss: 7.6753 - val_binary_accuracy: 0.9859\n",
            "Epoch 3/10\n",
            "43461/43461 [==============================] - 2s 42us/step - loss: 5.0757 - binary_accuracy: 0.9915 - val_loss: 6.3181 - val_binary_accuracy: 0.9954\n",
            "Epoch 4/10\n",
            "43461/43461 [==============================] - 2s 42us/step - loss: 3.8092 - binary_accuracy: 0.9937 - val_loss: 4.4588 - val_binary_accuracy: 0.9948\n",
            "Epoch 5/10\n",
            "43461/43461 [==============================] - 2s 42us/step - loss: 2.4137 - binary_accuracy: 0.9957 - val_loss: 5.2061 - val_binary_accuracy: 0.9961\n",
            "Epoch 6/10\n",
            "43461/43461 [==============================] - 2s 43us/step - loss: 1.9559 - binary_accuracy: 0.9964 - val_loss: 5.1953 - val_binary_accuracy: 0.9961\n",
            "Epoch 7/10\n",
            "43461/43461 [==============================] - 2s 43us/step - loss: 1.2849 - binary_accuracy: 0.9979 - val_loss: 4.9977 - val_binary_accuracy: 0.9967\n",
            "Epoch 8/10\n",
            "43461/43461 [==============================] - 2s 42us/step - loss: 1.1510 - binary_accuracy: 0.9979 - val_loss: 5.5850 - val_binary_accuracy: 0.9969\n",
            "Epoch 9/10\n",
            "43461/43461 [==============================] - 2s 42us/step - loss: 0.7412 - binary_accuracy: 0.9985 - val_loss: 6.5791 - val_binary_accuracy: 0.9979\n",
            "Epoch 10/10\n",
            "43461/43461 [==============================] - 2s 42us/step - loss: 1.4869 - binary_accuracy: 0.9974 - val_loss: 6.7924 - val_binary_accuracy: 0.9954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd2aafcbac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjcJPMEKFLSK",
        "colab_type": "code",
        "outputId": "9835ccd0-7d74-40a3-d2a0-56d6baae9ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "def binary_real_world_cost(fn, fp, fn_cost, fp_cost):\n",
        "  return fn*fn_cost + fp*fp_cost\n",
        "\n",
        "def binary_stats(z, y, threshold, fn_cost, fp_cost):\n",
        "  z = np.squeeze(z)\n",
        "\n",
        "  y_pred = z > threshold\n",
        "  p = np.count_nonzero(y)\n",
        "  n = np.count_nonzero(1.-y)\n",
        "\n",
        "  tp = np.count_nonzero(y_pred * y)\n",
        "  fn = np.count_nonzero((1.-y_pred) * y)\n",
        "  tn = np.count_nonzero((1.-y_pred) * (1.-y))\n",
        "  fp = np.count_nonzero(y_pred * (1.-y))\n",
        "\n",
        "  recall = np.float64(tp) / (tp + fn) \n",
        "  precision = np.float64(tp) / (tp + fp)\n",
        "  f1 = np.float64(2 * recall * precision) / (recall + precision)\n",
        "  rwc = (binary_real_world_cost (fn, fp, fn_cost, fp_cost)) / (p+n)\n",
        "  rwb = (p * fn_cost) / (p+n) - rwc\n",
        "  \n",
        "  return p, n, tp, fn, tn, fp, recall, precision, f1, rwc, rwb\n",
        "\n",
        "# This is the algorithm to fine tune the threshold hyperparameter after training. \n",
        "def search_threshold(z, y):\n",
        "  best_t = 0;\n",
        "  best_f1 = 0\n",
        "\n",
        "  data = []\n",
        "  \n",
        "  for i in np.linspace(0.01, .99, num=100):\n",
        "    _, _, _, _, _, _, _, _, f1, _, _ = binary_stats(z, y, i, 0, 0)\n",
        "    data.append((i, f1))\n",
        "    if f1 > best_f1:\n",
        "      best_f1 = f1\n",
        "      best_t = i  \n",
        "  return best_t, data\n",
        "\n",
        "y_pred_c = model_control.predict(X_test)\n",
        "y_pred_t = model_test.predict(X_test)\n",
        "\n",
        "stats_control = binary_stats(y_pred_c, y_test, 0.5, FN_MARGINAL_COST, FP_MARGINAL_COST)\n",
        "best_threshold, data = search_threshold(y_pred_c, y_test)\n",
        "stats_control_adj = binary_stats(y_pred_c, y_test, best_threshold, FN_MARGINAL_COST, FP_MARGINAL_COST)\n",
        "stats_test = binary_stats(y_pred_t, y_test, 0.5, FN_MARGINAL_COST, FP_MARGINAL_COST)\n",
        "\n",
        "print(tabulate([('Control',) + stats_control, \\\n",
        "                ('Control_adj',) + stats_control_adj, \\\n",
        "                ('Experimental',) + stats_test], \\\n",
        "               headers=['P', 'N', 'TP', \"FN\", \"TN\", \"FP\", \"Recall\", \"Precision\", \"F1\", \"Real World Cost\", \"Real World Benefit\"], floatfmt=\".6f\"))\n",
        "\n",
        "plt.plot(np.array(data)[:,0], np.array(data)[:,1])\n",
        "plt.title('F1 Score by Threshhold')\n",
        "# ax.set(xlabel='Threshold', ylabel='F1',\n",
        "#        title='F1 Score by Threshold')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                P      N    TP    FN     TN    FP    Recall    Precision        F1    Real World Cost    Real World Benefit\n",
            "------------  ---  -----  ----  ----  -----  ----  --------  -----------  --------  -----------------  --------------------\n",
            "Control       180  15917   147    33  15909     8  0.816667     0.948387  0.877612           4.149842             18.214574\n",
            "Control_adj   180  15917   154    26  15906    11  0.855556     0.933333  0.892754           3.298751             19.065664\n",
            "Experimental  180  15917   169    11  15843    74  0.938889     0.695473  0.799054           1.826427             20.537988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'F1 Score by Threshhold')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHEB51REPE6U",
        "colab_type": "text"
      },
      "source": [
        "# Graphical Display of Loss Function Mechanics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VASnHSpPPlc",
        "colab_type": "code",
        "outputId": "e031cd0e-d95a-43ce-dfe6-b5e71a265290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "source": [
        "def J(z, y):\n",
        "  if (y==1):\n",
        "    return -np.log(z)\n",
        "  elif (y==0):\n",
        "    return -np.log(1-z)\n",
        "  else:\n",
        "    assert false\n",
        "\n",
        "def J_batch(p, n_h, n_t):\n",
        "  m = n_h + n_t\n",
        "  x = np.zeros(m)\n",
        "\n",
        "  for i in range(0,n_h):\n",
        "    x[i] = 1\n",
        "\n",
        "  retval = 0.\n",
        "  for i in range(0, m):\n",
        "    retval += J(p, x[i])\n",
        "  retval /= x.shape[0]\n",
        "  return retval\n",
        "\n",
        "x_and_y = []\n",
        "    \n",
        "for i in np.linspace(0.0001, 0.9999, 1000):\n",
        "  x_and_y.append((i, J_batch(i, 9, 1)))\n",
        "\n",
        "x_and_y = np.array(x_and_y)\n",
        "\n",
        "print(x_and_y.shape)\n",
        "  \n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(x_and_y[:,0], x_and_y[:,1])\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAHjCAYAAAAUtNr0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nem0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2EfTMy9YS1q",
        "colab_type": "text"
      },
      "source": [
        "# Bernoulli Trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWrXZk6VdYlW",
        "colab_type": "text"
      },
      "source": [
        "A quick Bernoulli trial. Bias should be close to 2.1. Sigmoid(2.1) = 0.9, which is the minimum cost / accurate prediction for 9 heads and 1 tail. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8faj_W8YcZm",
        "colab_type": "code",
        "outputId": "df5e1264-7026-439c-fbb7-dd8654f61458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "k_y = k_x = Input(shape=(1,))\n",
        "k_y = Dense(1, activation='sigmoid', kernel_initializer='zeros',\n",
        "                bias_initializer='ones')(k_y)\n",
        "\n",
        "model_bernoulli = Model(inputs=k_x, outputs=k_y)\n",
        "model_bernoulli.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "X = np.zeros((10,))\n",
        "y = np.concatenate([np.ones((9,)), np.zeros((1,))])\n",
        "\n",
        "model_bernoulli.fit(x = X, y = y, epochs=1000, verbose=0) \n",
        "print(model_bernoulli.evaluate(x = X, y = y))\n",
        "print(model_bernoulli.get_weights()[1])\n",
        "\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "\n",
        "print(sigmoid(model_bernoulli.get_weights()[1][0]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r10/10 [==============================] - 1s 57ms/step\n",
            "[0.33526673913002014, 0.8999999761581421]\n",
            "[1.75]\n",
            "0.8518410365873759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtdkeX4kdJLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}